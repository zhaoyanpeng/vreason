use_lars: False 
name: AdamW
warmup: False 
warmup_steps: 1000
warmup_times: ${.warmup_steps} 
warmup_epoch: 10
lr: 3e-3
weight_decay: 0. 
betas: [0.9, 0.95]
max_gnorm: 1.0 
lr_weight: 0.2
lr_bias: 0.0048
batch_size: ${running.batch_size}
epochs: ${running.epochs}
steps: []
gamma: 0.5
batch_sch: False # schedule lr per batch
optimizer: [AdamW, {lr: '${optimizer.lr}', betas: '${optimizer.betas}'}]
scheduler: [MultiStepLR, {milestones: '${optimizer.steps}', gamma: '${optimizer.gamma}'}]
