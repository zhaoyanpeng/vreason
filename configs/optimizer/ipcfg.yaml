use_lars: false 
name: ${.opt_name}
warmup: false 
warmup_steps: 10000
warmup_times: ${.warmup_steps} 
warmup_epoch: 10
warmup_fn: "linear" 
decay_step: 100000
decay_rate: .5
lr: 1e-4
weight_decay: 0. 
betas: [0.99, 0.95]
max_gnorm: 3.0 
lr_weight: 0.2
lr_bias: 0.0048
batch_size: ${running.batch_size}
epochs: ${running.epochs}
steps: []
gamma: 0.5
batch_sch: true # schedule lr per batch
opt_name: AdamW
sch_name: WarmupExpDecayLR 
optimizer: ['${optimizer.opt_name}',{lr: '${optimizer.lr}', betas: '${optimizer.betas}'}]
scheduler: ['${optimizer.sch_name}',{step_size: '${optimizer.decay_step}', gamma: '${optimizer.decay_rate}', warmup_step: '${optimizer.warmup_steps}', warmup_fn: '${optimizer.warmup_fn}'}]
